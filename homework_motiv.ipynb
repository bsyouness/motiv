{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motiv Data Scientist/Algorithm Engineer Interview Assignment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disclaimer**: This is loosely built around a [machine learning crash course](https://developers.google.com/machine-learning/crash-course/first-steps-with-tensorflow/programming-exercises) and [TensorFlow tutorial](https://www.tensorflow.org/get_started/get_started_for_beginners)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and modify the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firt things first, import the modules we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at what's inside our zip file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_folder = zipfile.ZipFile(\"Algorithm_DSdata.zip\", \"r\")\n",
    "zip_folder.namelist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at what the first file look like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_file = zip_folder.namelist()[1]\n",
    "zip_folder.read(first_file)[:100].decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're told that the data is as follows:\n",
    "1. UTC. The data is sampled roughly at 50Hz. Feel free to assume all data was\n",
    "uniformly at 50Hz, but please be prepared to discuss the ramification of this\n",
    "assumption.\n",
    "2. Acceleration in g’s for the x-axis of an accelerometer. Values will be positive or\n",
    "negative.\n",
    "3. Acceleration in g’s for the y-axis of an accelerometer. Values will be positive or\n",
    "negative.\n",
    "4. Acceleration in g’s for the z-axis of an accelerometer. Values will be positive or\n",
    "negative.\n",
    "5. Activity label. This element will either be blank or contain either “walk” or “run”. Any\n",
    "element with a blank label is considered to be neither walking nor running.\n",
    "\n",
    "Considering that, let's load an aggregated dataframe (the files aren't too large, so it shouldn't be a problem to keep that in memory). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = 'utc, x, y, z, label'.split(', ')\n",
    "df_list = []\n",
    "\n",
    "for file_name in zip_folder.namelist()[1:]:\n",
    "    csv = StringIO(zip_folder.read(file_name).decode(\"utf-8\"))\n",
    "    data = pd.read_csv(csv, header=None, names=column_names, encoding=\"utf-8\")    \n",
    "    df_list.append(data)\n",
    "\n",
    "zip_folder.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what one of our dataframes looks like. It's not surprising that the label contains a lot of `NaN`s..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_list[0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lr6wYl2bt2Ep",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Build the model\n",
    "\n",
    "In this exercise, we'll try to predict whether a record corresponds to `Walk` or `Run`, which will be our label. We'll use `x`, `y` and `z` as our input features.\n",
    "\n",
    "To train our model, we'll use the TensorFlow [Estimator](https://www.tensorflow.org/get_started/estimator) API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0cpcsieFhsNI"
   },
   "source": [
    "### Step 1: Define features and configure feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "rhEbFCZ86cDZ"
   },
   "outputs": [],
   "source": [
    "# Define the input features.\n",
    "feature_columns =  'x, y, z'.split(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can make some rows non-contiguous temporally.\n",
    "labeled_list, unlabeled_list = (\n",
    "    [df.loc[~df.label.isna(), feature_columns + ['label']] for df in df_list], \n",
    "    [df.loc[df.label.isna(), feature_columns + ['label']] for df in df_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to convert the label to an integer to train our model, might as well do that here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in labeled_list:\n",
    "    df.label.replace({'Walk': 0, 'Run': 1}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_list[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Our label in now an `int`, we can pursue...\n",
    "\n",
    "**Note**: Some of our files contained only unlabeled data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(df) for df in labeled_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Only 2 of our *csv* files contain both *Walk* and *Run* labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(i, df.label.unique()) for (i, df) in enumerate(labeled_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our training, validation and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ts = labeled_list[0:3] + labeled_list[5:7] + [labeled_list[8]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KoDaF2dlJQG5"
   },
   "source": [
    "Let's make predictions on that training data, to see how well our model fit it during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(labeled_list[3].label)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wanted to split `labeled_list[3]` into validation and testing data, but because of the lack of variability in the data, we'll just keep one testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ts = [labeled_list[3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-0IztwdK2f3F"
   },
   "source": [
    "### Step 2: Define the input function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S5M5j6xSCHxx"
   },
   "source": [
    "The input function instructs TensorFlow how to preprocess\n",
    "the data, as well as how to batch, shuffle, and repeat it during model training.\n",
    "\n",
    "First, we'll convert our *pandas* feature data into a dict of NumPy arrays. We can then use the TensorFlow [Dataset API](https://www.tensorflow.org/programmers_guide/datasets) to construct a dataset object from our data, and then break\n",
    "our data into batches of `batch_size`, to be repeated for the specified number of epochs (num_epochs). \n",
    "\n",
    "**NOTE:** When the default value of `num_epochs=None` is passed to `repeat()`, the input data will be repeated indefinitely.\n",
    "\n",
    "Next, if `shuffle` is set to `True`, we'll shuffle the data so that it's passed to the model randomly during training. The `buffer_size` argument specifies\n",
    "the size of the dataset from which `shuffle` will randomly sample.\n",
    "\n",
    "Finally, our input function constructs an iterator for the dataset and returns the next batch of data to our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "RKZ9zNcHJtwc"
   },
   "outputs": [],
   "source": [
    "NUM_FEATURES = 3\n",
    "# The classifier should output its class predictions every 50 samples (roughly 1Hz).\n",
    "NUM_POINTS = 50\n",
    "\n",
    "def timeseries_to_tf_dataset(single_timeseries):\n",
    "    assert(isinstance(single_timeseries, pd.DataFrame))\n",
    "    # The columns are acceleration on the x, y and z axis, and the labels. \n",
    "    assert(len(single_timeseries.columns) == NUM_FEATURES+1)\n",
    "    \n",
    "    def crop_and_slice(tensor):\n",
    "        # Select a random fraction of our dataset. \n",
    "        tensor = tf.random_crop(tensor, [NUM_POINTS, NUM_FEATURES+1])\n",
    "        features = tf.reshape(tensor[:, :NUM_FEATURES], [NUM_FEATURES*NUM_POINTS])\n",
    "        targets = tensor[-1, NUM_FEATURES]\n",
    "        return features, targets\n",
    "\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensors(np.array(single_timeseries)).repeat()\n",
    "    return dataset.map(crop_and_slice)\n",
    "\n",
    "# Currently not working. \n",
    "def sample_from_datasets(datasets, weights):\n",
    "    def sample(*ds_samples):\n",
    "        # An integer in [0, len(datasets))\n",
    "        index = tf.random_uniform((), 0, len(datasets), dtype=tf.int32)\n",
    "        return ds_samples[index]\n",
    "    \n",
    "    return tf.data.Dataset.zip(datasets).map(sample)\n",
    "\n",
    "# Source: \n",
    "# https://stackoverflow.com/questions/49058913/interleaving-multiple-tensorflow-datasets-together\n",
    "def combine_datasets(datasets):\n",
    "    def concat(xs):\n",
    "        xs0 = tf.data.Dataset.from_tensors(xs[0])\n",
    "        for x in xs[1:]:\n",
    "            xs0 = xs0.concatenate(tf.data.Dataset.from_tensors(x))\n",
    "        return xs0\n",
    "    \n",
    "    return tf.data.Dataset.zip(datasets).flat_map(lambda *xs: concat(xs))\n",
    "    \n",
    "def input_fn(timeseries_list, batch_size, shuffle=False):\n",
    "    \"\"\"An input function for training.\n",
    "\n",
    "    Args:\n",
    "      timeseries_list: list of pandas DataFrames of features and targets\n",
    "      batch_size: Size of batches to be passed to the model\n",
    "      shuffle: True or False. Whether to shuffle the data.\n",
    "    Returns:\n",
    "      Tensorflow dataset\n",
    "    \"\"\"\n",
    "    datasets, weights = zip(*[(timeseries_to_tf_dataset(x), len(x)) for x in timeseries_list])\n",
    "    # We're currently not using the weights.\n",
    "    weights = [float(w)/sum(weights) for w in weights]\n",
    "    dataset = combine_datasets(datasets)\n",
    "\n",
    "    # Turn the dataset feature into a dictionary.\n",
    "    def make_features_a_dictionary(feature, label):\n",
    "        return {'combined_features': feature}, label\n",
    "    dataset = dataset.map(make_features_a_dictionary)\n",
    "    \n",
    "    # Shuffle the data, if specified.\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=10000)\n",
    "\n",
    "    #  and batch the examples.\n",
    "    dataset = dataset.apply(tf.contrib.data.batch_and_drop_remainder(batch_size))\n",
    "\n",
    "    # Return the dataset.\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4YS50CQb2ooO"
   },
   "source": [
    "### Step 3: Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build 2 hidden layer DNN.\n",
    "my_feature_columns = [tf.feature_column.numeric_column(key='combined_features', shape=(NUM_FEATURES * NUM_POINTS,))]\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=my_feature_columns,\n",
    "    hidden_units=[50, 50],\n",
    "    n_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4YS50CQb2ooO"
   },
   "source": [
    "### Step 4: Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yP92XkzhU803"
   },
   "source": [
    "First we need to separate training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNN parameters.\n",
    "batch_size = 100\n",
    "train_steps = 1000\n",
    "\n",
    "# Train the Model.\n",
    "classifier.train(\n",
    "    input_fn=lambda:input_fn(train_ts, batch_size), steps=train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Nwxqxlx2sOv"
   },
   "source": [
    "### Step 5: Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.evaluate(\n",
    "    input_fn=lambda:input_fn(train_ts, batch_size), steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.evaluate(\n",
    "    input_fn=lambda:input_fn(test_ts, batch_size), steps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KoDaF2dlJQG5"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "The trained model is nearly perfect on the train set, which contains both *Walk* and *Run* records, but predicts *Walk* for every example in the test set. This looks like overfitting, but we tried some standard things (reducing model complexity or training time), with no results. It's possible there's a big qualitative difference between the train and test sets."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "JndnmDMp66FL",
    "ajVM7rkoYXeL",
    "ci1ISxxrZ7v0"
   ],
   "default_view": {},
   "name": "first_steps_with_tensor_flow.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
